{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ebf6747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision timm scikit-learn pillow matplotlib seaborn tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06d20eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "↪ Resuming from epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 1601/1601 [12:39<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc=0.9966  best=0.9966  patience=2\n",
      "✓ Metrics saved → evaluation_results\\fastvit_t8_metrics.json\n",
      "✓ Confusion matrix plot saved.\n",
      "Finished! Checkpoints in trained_models\\fastvit_t8\n"
     ]
    }
   ],
   "source": [
    "# run_fastvit_t8_highres_v3.py\n",
    "# Final high-resolution experiment for 24 IMC paper (with robust resume for scheduler)\n",
    "# Author: <your-name>          Date: 2025-08-xx\n",
    "\n",
    "import warnings, random, json, time\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import timm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, confusion_matrix, classification_report)\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 1. CONFIGURATION\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "MODEL_NAME     = \"fastvit_t8\"\n",
    "ORIG_DATA_DIR  = Path(\"../datasets\")\n",
    "CKPT_DIR       = Path(\"trained_models\")/MODEL_NAME\n",
    "LOG_CSV        = Path(\"logs\")/f\"{MODEL_NAME}.csv\"\n",
    "OUT_JSON       = Path(\"evaluation_results\")/f\"{MODEL_NAME}_metrics.json\"\n",
    "\n",
    "IMG_SIZE       = 224\n",
    "BATCH          = 8\n",
    "ACC_STEPS      = 4\n",
    "FROZEN_EPOCHS  = 5\n",
    "TOTAL_EPOCHS   = 20\n",
    "PATIENCE       = 5\n",
    "LR             = 1e-4\n",
    "WD             = 1e-4\n",
    "WORKERS        = 4\n",
    "SEED           = 42\n",
    "USE_COMPILE    = False\n",
    "DEVICE         = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "AMP            = torch.cuda.is_available()\n",
    "\n",
    "for p in [\"trained_models\", \"logs\", \"evaluation_results\"]:\n",
    "    Path(p).mkdir(exist_ok=True)\n",
    "\n",
    "torch.manual_seed(SEED); np.random.seed(SEED); random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 2. DATA TRANSFORMS & LOADERS\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "def build_transforms():\n",
    "    norm = transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    train_tf = transforms.Compose([transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8,1.0)), transforms.RandomHorizontalFlip(), transforms.ToTensor(), norm])\n",
    "    val_tf   = transforms.Compose([transforms.Resize(int(IMG_SIZE*1.12)), transforms.CenterCrop(IMG_SIZE), transforms.ToTensor(), norm])\n",
    "    return train_tf, val_tf\n",
    "\n",
    "full_ds = datasets.ImageFolder(ORIG_DATA_DIR)\n",
    "CLASS_NAMES = full_ds.classes\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "train_idx, val_idx = train_test_split(np.arange(len(full_ds.targets)), test_size=0.2, stratify=full_ds.targets, random_state=SEED)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 3. HELPERS\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "def freeze_backbone(model, train_full=False):\n",
    "    for n,p in model.named_parameters():\n",
    "        p.requires_grad = train_full or any(k in n for k in (\"head\",\"fc\",\"classifier\"))\n",
    "\n",
    "def maybe_compile(m):\n",
    "    if USE_COMPILE and hasattr(torch,\"compile\"):\n",
    "        try: return torch.compile(m, dynamic=False)\n",
    "        except Exception as e: print(\"torch.compile disabled →\", e)\n",
    "    return m\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval(); preds=[]; labels=[]\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(enabled=AMP):\n",
    "        for xb,yb in loader:\n",
    "            preds.extend(model(xb.to(DEVICE)).argmax(1).cpu().numpy())\n",
    "            labels.extend(yb.numpy())\n",
    "    return np.array(preds), np.array(labels)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 4. TRAINING LOOP\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    train_tf, val_tf = build_transforms()\n",
    "    train_ds = Subset(full_ds, train_idx); train_ds.dataset.transform = train_tf\n",
    "    val_ds   = Subset(full_ds, val_idx);   val_ds.dataset.transform   = val_tf\n",
    "    train_ld = DataLoader(train_ds, BATCH, True , num_workers=WORKERS, pin_memory=True)\n",
    "    val_ld   = DataLoader(val_ds,   BATCH, False, num_workers=WORKERS, pin_memory=True)\n",
    "\n",
    "    model = timm.create_model(MODEL_NAME, pretrained=True, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=AMP)\n",
    "\n",
    "    start_ep, best_acc, patience_left = 0, 0., PATIENCE\n",
    "    optimizer_state, scaler_state, scheduler_state = None, None, None # --- MODIFIED ---\n",
    "\n",
    "    if (CKPT_DIR/\"last.pth\").is_file():\n",
    "        chk = torch.load(CKPT_DIR/\"last.pth\", map_location=DEVICE)\n",
    "        model.load_state_dict(chk[\"model\"])\n",
    "        start_ep       = chk[\"epoch\"] + 1\n",
    "        best_acc       = chk[\"best_acc\"]\n",
    "        patience_left  = chk[\"patience\"]\n",
    "        optimizer_state = chk[\"optim\"]\n",
    "        scaler_state    = chk[\"scaler\"]\n",
    "        scheduler_state = chk.get(\"scheduler\") # --- MODIFIED --- .get() is safer\n",
    "        print(f\"↪ Resuming from epoch {start_ep}\")\n",
    "\n",
    "    if start_ep < FROZEN_EPOCHS:\n",
    "        freeze_backbone(model, False)\n",
    "        optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR, weight_decay=WD)\n",
    "    else:\n",
    "        freeze_backbone(model, True)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=LR*0.5, weight_decay=WD)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TOTAL_EPOCHS) # --- MODIFIED --- No last_epoch needed\n",
    "\n",
    "    if optimizer_state: optimizer.load_state_dict(optimizer_state)\n",
    "    if scaler_state: scaler.load_state_dict(scaler_state)\n",
    "    if scheduler_state: scheduler.load_state_dict(scheduler_state) # --- MODIFIED ---\n",
    "\n",
    "    model = maybe_compile(model)\n",
    "\n",
    "    if start_ep==0 and LOG_CSV.exists(): LOG_CSV.unlink()\n",
    "\n",
    "    for ep in range(start_ep, TOTAL_EPOCHS):\n",
    "        if ep == FROZEN_EPOCHS:\n",
    "            freeze_backbone(model, True)\n",
    "            optimizer = optim.AdamW(model.parameters(), lr=LR*0.5, weight_decay=WD)\n",
    "            # Recreate scheduler for the new optimizer\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TOTAL_EPOCHS, last_epoch=ep-1)\n",
    "\n",
    "        model.train(); seen=0; loss_sum=0; correct=0\n",
    "        optimizer.zero_grad()\n",
    "        for i,(xb,yb) in enumerate(tqdm(train_ld, desc=f\"Epoch {ep+1}/{TOTAL_EPOCHS}\")):\n",
    "            xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            with torch.cuda.amp.autocast(enabled=AMP):\n",
    "                out = model(xb); loss = criterion(out,yb)/ACC_STEPS\n",
    "            scaler.scale(loss).backward()\n",
    "            if (i+1)%ACC_STEPS==0:\n",
    "                scaler.step(optimizer); scaler.update(); optimizer.zero_grad()\n",
    "            loss_sum += loss.item()*ACC_STEPS*xb.size(0)\n",
    "            correct  += (out.argmax(1)==yb).sum().item()\n",
    "            seen     += xb.size(0)\n",
    "        train_loss, train_acc = loss_sum/seen, correct/seen\n",
    "\n",
    "        preds, labels = evaluate(model, val_ld)\n",
    "        val_acc = accuracy_score(labels, preds)\n",
    "        scheduler.step()\n",
    "\n",
    "        with open(LOG_CSV,\"a\") as f:\n",
    "            if ep==0: f.write(\"epoch,train_loss,train_acc,val_acc,lr\\n\")\n",
    "            f.write(f\"{ep},{train_loss:.5f},{train_acc:.5f},{val_acc:.5f},{scheduler.get_last_lr()[0]:.6f}\\n\")\n",
    "        print(f\"val_acc={val_acc:.4f}  best={best_acc:.4f}  patience={patience_left}\")\n",
    "\n",
    "        # --- MODIFIED ---: Save scheduler state in checkpoint\n",
    "        state = {\"epoch\":ep,\"model\":model.state_dict(),\"optim\":optimizer.state_dict(),\n",
    "                 \"scaler\":scaler.state_dict(),\"scheduler\":scheduler.state_dict(),\n",
    "                 \"best_acc\":best_acc,\"patience\":patience_left}\n",
    "        CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(state, CKPT_DIR/\"last.pth\")\n",
    "        torch.save(state, CKPT_DIR/f\"epoch{ep:03d}.pth\")\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            patience_left = PATIENCE\n",
    "            torch.save(state, CKPT_DIR/\"best.pth\")\n",
    "        else:\n",
    "            patience_left -= 1\n",
    "        \n",
    "        if patience_left == 0 and ep >= FROZEN_EPOCHS:\n",
    "            print(\"Early-stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    # Final Evaluation\n",
    "    best_state = torch.load(CKPT_DIR/\"best.pth\", map_location=DEVICE)\n",
    "    model.load_state_dict(best_state[\"model\"])\n",
    "    preds, labels = evaluate(model, val_ld)\n",
    "    t0=time.time()\n",
    "    with torch.no_grad():\n",
    "        for xb,_ in val_ld: model(xb.to(DEVICE))\n",
    "    latency = (time.time()-t0)/len(val_ds)\n",
    "    metrics = dict(model=MODEL_NAME, image_size=IMG_SIZE, val_accuracy=accuracy_score(labels, preds), precision=precision_score(labels, preds, average=\"weighted\", zero_division=0), recall=recall_score(labels, preds, average=\"weighted\",  zero_division=0), f1_score=f1_score(labels, preds,  average=\"weighted\", zero_division=0), inf_sec_per_img=latency, conf_matrix=confusion_matrix(labels, preds).tolist(), class_report=classification_report(labels, preds, target_names=CLASS_NAMES, zero_division=0, output_dict=True))\n",
    "    OUT_JSON.parent.mkdir(exist_ok=True)\n",
    "    json.dump(metrics, open(OUT_JSON,\"w\"), indent=2)\n",
    "    print(\"✓ Metrics saved →\", OUT_JSON)\n",
    "    sns.heatmap(metrics[\"conf_matrix\"], cmap=\"Blues\", cbar=False, annot=False, xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
    "    plt.title(f\"{MODEL_NAME} Confusion Matrix\"); plt.tight_layout()\n",
    "    plt.savefig(OUT_JSON.with_suffix(\".png\"), dpi=300); plt.close()\n",
    "    print(\"✓ Confusion matrix plot saved.\")\n",
    "    print(\"Finished! Checkpoints in\", CKPT_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6877b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
